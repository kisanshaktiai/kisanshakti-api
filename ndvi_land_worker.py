#!/usr/bin/env python3
# ndvi_land_worker_v10.0.py â€” Enhanced for Small Landholdings
# v10.0 â€” Accurate NDVI for micro-parcels with quality scoring and audit compliance
# Features: Geometry buffering, bilinear interpolation, multi-temporal compositing,
#          comprehensive quality metrics, and regulatory compliance documentation

import os
import io
import json
import time
import logging
import datetime
import traceback
import argparse
from typing import List, Optional, Dict, Any, Tuple
from dataclasses import dataclass

import asyncio
from concurrent.futures import ThreadPoolExecutor

import numpy as np
import rasterio
from rasterio.mask import mask
from rasterio.errors import RasterioIOError
from rasterio.warp import transform_geom, reproject, Resampling
from rasterio.crs import CRS
from rasterio.transform import from_bounds
from shapely.geometry import shape, mapping, box
from shapely.ops import transform as shapely_transform
from shapely import wkb
from pyproj import Transformer
from PIL import Image
import matplotlib.cm as cm

from supabase import create_client, Client
from b2sdk.v2 import InMemoryAccountInfo, B2Api

# ---------------- Logging ----------------
logging.basicConfig(
    level=os.getenv("LOG_LEVEL", "INFO"),
    format="%(asctime)s | %(levelname)-7s | %(name)s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger("ndvi-worker-v10.0")

# ---------------- Configuration / Env ----------------
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_SERVICE_ROLE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
B2_KEY_ID = os.getenv("B2_KEY_ID")
B2_APP_KEY = os.getenv("B2_APP_KEY")
B2_BUCKET_NAME = os.getenv("B2_BUCKET_NAME", "kisanshakti-ndvi-tiles")
B2_PUBLIC_REGION = os.getenv("B2_PUBLIC_REGION", "f005")
SUPABASE_NDVI_BUCKET = os.getenv("SUPABASE_NDVI_BUCKET", "ndvi-thumbnails")

MAX_CONCURRENT_LANDS = int(os.getenv("MAX_CONCURRENT_LANDS", "6"))
THREAD_POOL_WORKERS = int(os.getenv("THREAD_POOL_WORKERS", "8"))

# Small parcel optimization settings
GEOMETRY_BUFFER_METERS = float(os.getenv("GEOMETRY_BUFFER_METERS", "15.0"))  # 1.5 pixels at 10m
MIN_VALID_PIXELS = int(os.getenv("MIN_VALID_PIXELS", "4"))  # Minimum for calculation
ENABLE_MULTI_TEMPORAL = os.getenv("ENABLE_MULTI_TEMPORAL", "true").lower() == "true"
TEMPORAL_LOOKBACK_DAYS = int(os.getenv("TEMPORAL_LOOKBACK_DAYS", "30"))
MAX_TEMPORAL_IMAGES = int(os.getenv("MAX_TEMPORAL_IMAGES", "3"))

# Quality thresholds
CONFIDENCE_HIGH_PIXELS = int(os.getenv("CONFIDENCE_HIGH_PIXELS", "100"))
CONFIDENCE_MEDIUM_PIXELS = int(os.getenv("CONFIDENCE_MEDIUM_PIXELS", "25"))

# Validate critical env
if not all([SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY, B2_KEY_ID, B2_APP_KEY]):
    logger.critical("Missing required environment variables: SUPABASE_* or B2_*")
    raise RuntimeError("Missing required environment variables")

# ---------------- Clients ----------------
supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY)

b2_info = InMemoryAccountInfo()
b2_api = B2Api(b2_info)
b2_api.authorize_account("production", B2_KEY_ID, B2_APP_KEY)
try:
    b2_bucket = b2_api.get_bucket_by_name(B2_BUCKET_NAME)
    logger.info(f"âœ… B2 bucket connected: {B2_BUCKET_NAME}")
except Exception as e:
    logger.critical(f"âŒ B2 bucket access failed: {e}")
    b2_bucket = None

# ---------------- Data Classes ----------------
@dataclass
class QualityMetrics:
    """Quality assessment for NDVI calculation"""
    confidence_level: str  # High, Medium, Low
    pixel_count: int
    edge_pixel_ratio: float
    spatial_coverage: float
    temporal_consistency: Optional[float]
    cloud_contamination: float
    quality_score: float  # 0-100
    quality_flags: List[str]
    data_source: str
    processing_method: str
    uncertainty_estimate: float

@dataclass
class NDVIResult:
    """Complete NDVI extraction result with metadata"""
    ndvi_array: np.ndarray
    statistics: Dict[str, Any]
    quality: QualityMetrics
    metadata: Dict[str, Any]
    acquisition_dates: List[str]

# ---------------- Utilities ----------------
def now_iso() -> str:
    return datetime.datetime.utcnow().isoformat() + "Z"

def safe_json_load(s: Any) -> Optional[Dict]:
    try:
        if s is None:
            return None
        if isinstance(s, dict):
            return s
        if isinstance(s, str):
            return json.loads(s)
        return None
    except Exception:
        return None

# ---------------- Signed B2 URL logic ----------------
def validate_http_url(url: str, timeout: int = 6) -> bool:
    """HEAD may be blocked on some endpoints; fall back to a small GET streaming request."""
    try:
        import requests
        try:
            r = requests.head(url, timeout=timeout, allow_redirects=True)
            if 200 <= r.status_code < 400:
                return True
        except Exception as e_head:
            logger.debug(f"HEAD failed for validation: {e_head} â€” trying GET stream")
            try:
                r = requests.get(url, stream=True, timeout=timeout, allow_redirects=True)
                if 200 <= r.status_code < 400:
                    return True
            except Exception as e_get:
                logger.debug(f"GET stream also failed during validation: {e_get}")
                return False
        return False
    except Exception as e:
        logger.debug(f"requests not available or validation failed: {e}")
        return False

def get_signed_b2_url(file_path: str, valid_secs: int = 3600) -> Optional[str]:
    """Generate a signed B2 URL that rasterio/GDAL can open. Returns None on failure."""
    if not b2_bucket:
        logger.error("B2 bucket client not available")
        return None

    try:
        auth_token = None
        for kwargs in (
            {"file_name_prefix": file_path, "valid_duration_in_seconds": valid_secs},
            {"file_name_prefix": file_path, "valid_duration_seconds": valid_secs},
            {"file_name_prefix": file_path, "valid_durationInSeconds": valid_secs},
        ):
            try:
                auth_token = b2_bucket.get_download_authorization(**kwargs)
                break
            except TypeError:
                continue
            except Exception as e:
                logger.debug(f"get_download_authorization attempt failed: {e}")
                continue

        if not auth_token:
            logger.warning(f"No auth token obtained for {file_path}")
            url_try = f"https://{B2_PUBLIC_REGION}.backblazeb2.com/file/{B2_BUCKET_NAME}/{file_path}"
            if validate_http_url(url_try):
                logger.debug(f"Public URL works for {file_path}")
                return url_try
            return None

        url = f"https://{B2_PUBLIC_REGION}.backblazeb2.com/file/{B2_BUCKET_NAME}/{file_path}?Authorization={auth_token}"
        if not validate_http_url(url):
            logger.warning(f"Signed B2 URL validation failed for {file_path}")
            return None
        return url

    except Exception as e:
        logger.exception(f"Failed to generate signed B2 URL for {file_path}: {e}")
        return None

# ---------------- Geometry helpers with buffering ----------------
def extract_geometry_from_land(land_record: Dict) -> Optional[Dict]:
    """Extract geometry GeoJSON in EPSG:4326 from a land record."""
    land_id = land_record.get("id", "<unknown>")
    
    for key in ("boundary", "boundary_geom"):
        val = land_record.get(key)
        if not val:
            continue
        try:
            if isinstance(val, str):
                s = val.strip()
                if s.startswith("{") or s.startswith("["):
                    return json.loads(s)
                if all(c in "0123456789abcdefABCDEF" for c in s.replace("0x", "")) and (len(s) % 2 == 0):
                    geom_bytes = bytes.fromhex(s.replace("0x", ""))
                else:
                    try:
                        return json.loads(s)
                    except Exception:
                        raise ValueError("String geometry not JSON nor hex WKB")
            elif isinstance(val, (bytes, bytearray)):
                geom_bytes = bytes(val)
            elif isinstance(val, dict):
                return val
            else:
                continue
            shapely_geom = wkb.loads(geom_bytes)
            geojson = mapping(shapely_geom)
            return geojson
        except Exception as e:
            logger.debug(f"land {land_id}: failed parse {key}: {e}")

    legacy = land_record.get("boundary_polygon_old")
    if legacy:
        try:
            if isinstance(legacy, dict):
                return legacy
            if isinstance(legacy, str):
                return json.loads(legacy)
        except Exception:
            logger.debug(f"land {land_id}: legacy polygon parse failed")

    logger.error(f"âŒ Land {land_id}: no valid geometry found")
    return None

def buffer_geometry_meters(geojson_4326: Dict, buffer_meters: float) -> Dict:
    """
    Buffer a geometry in EPSG:4326 by specified meters.
    Projects to appropriate UTM zone for accurate metric buffering.
    """
    try:
        geom = shape(geojson_4326)
        centroid = geom.centroid
        
        # Determine UTM zone
        lon = centroid.x
        lat = centroid.y
        utm_zone = int((lon + 180) / 6) + 1
        utm_epsg = 32600 + utm_zone if lat >= 0 else 32700 + utm_zone
        
        # Transform to UTM, buffer, transform back
        project_to_utm = Transformer.from_crs("EPSG:4326", f"EPSG:{utm_epsg}", always_xy=True)
        project_to_wgs84 = Transformer.from_crs(f"EPSG:{utm_epsg}", "EPSG:4326", always_xy=True)
        
        geom_utm = shapely_transform(project_to_utm.transform, geom)
        buffered_utm = geom_utm.buffer(buffer_meters)
        buffered_wgs84 = shapely_transform(project_to_wgs84.transform, buffered_utm)
        
        return mapping(buffered_wgs84)
    except Exception as e:
        logger.warning(f"Buffer operation failed: {e}, using original geometry")
        return geojson_4326

def reproject_to_raster_crs(geojson: Dict, target_crs: CRS) -> Dict:
    """Reproject a GeoJSON geometry (assumed EPSG:4326) to target_crs."""
    if geojson is None:
        raise ValueError("geojson is None")
    if isinstance(target_crs, CRS):
        target_crs_str = target_crs.to_string()
    else:
        target_crs_str = str(target_crs)
    try:
        reprojected = transform_geom("EPSG:4326", target_crs_str, geojson)
        if not reprojected or "coordinates" not in reprojected:
            raise ValueError("Invalid reprojection result")
        return reprojected
    except Exception as e:
        logger.error(f"Reprojection failed: {e}")
        raise

# ---------------- Enhanced rasterio operations ----------------
def _rasterio_open_with_retries(url: str, mode_kwargs: Dict = None, retries: int = 2, timeout: int = 30):
    """Small helper: rasterio.open on HTTP can fail transiently; do a couple retries."""
    attempt = 0
    mode_kwargs = mode_kwargs or {}
    while attempt <= retries:
        try:
            env_kwargs = {
                "GDAL_DISABLE_READDIR_ON_OPEN": "TRUE",
                "GDAL_HTTP_TIMEOUT": str(timeout),
                "GDAL_HTTP_MAX_RETRY": "3",
                "GDAL_HTTP_RETRY_DELAY": "1",
            }
            env = rasterio.Env(**env_kwargs)
            env.__enter__()
            src = rasterio.open(url, **mode_kwargs)
            return src, env
        except RasterioIOError as e:
            attempt += 1
            logger.debug(f"rasterio open attempt {attempt} failed for {url}: {e}")
            if attempt > retries:
                logger.exception(f"Rasterio open final failure for {url}")
                raise
            time.sleep(1 + attempt)
        except Exception as e:
            attempt += 1
            logger.debug(f"rasterio.open unexpected error (attempt {attempt}) for {url}: {e}")
            if attempt > retries:
                raise
            time.sleep(0.5)
    raise RuntimeError("rasterio open retries exhausted")

def extract_with_interpolation(
    src: rasterio.DatasetReader,
    geometry_projected: Dict,
    use_bilinear: bool = True
) -> Tuple[np.ndarray, Any]:
    """
    Extract data from raster with optional bilinear interpolation for better accuracy.
    For small parcels, upsampling with bilinear helps capture partial pixels.
    """
    try:
        land_shape = shape(geometry_projected)
        tile_bbox = box(*src.bounds)
        
        if not land_shape.intersects(tile_bbox):
            raise ValueError("No spatial overlap")
        
        # For very small parcels, upsample 2x with bilinear before extraction
        if use_bilinear and land_shape.area < (src.res[0] * src.res[1] * 100):  # < 100 pixels
            logger.debug("Using bilinear upsampling for small parcel")
            
            # Calculate upsampled dimensions
            bounds = land_shape.bounds
            width = int((bounds[2] - bounds[0]) / (src.res[0] / 2)) + 20  # Extra margin
            height = int((bounds[3] - bounds[1]) / (src.res[1] / 2)) + 20
            
            # Create destination array
            dst_array = np.empty((src.count, height, width), dtype=np.float32)
            dst_transform = from_bounds(*bounds, width, height)
            
            # Reproject with bilinear
            reproject(
                source=rasterio.band(src, 1),
                destination=dst_array[0],
                src_transform=src.transform,
                src_crs=src.crs,
                dst_transform=dst_transform,
                dst_crs=src.crs,
                resampling=Resampling.bilinear
            )
            
            # Now mask from upsampled array
            from rasterio.io import MemoryFile
            with MemoryFile() as memfile:
                with memfile.open(
                    driver='GTiff',
                    height=height,
                    width=width,
                    count=1,
                    dtype=np.float32,
                    crs=src.crs,
                    transform=dst_transform,
                ) as temp_src:
                    temp_src.write(dst_array[0], 1)
                    clip, transform = mask(temp_src, [geometry_projected], crop=True, all_touched=True, indexes=1)
        else:
            # Standard extraction with all_touched for edge pixels
            clip, transform = mask(src, [geometry_projected], crop=True, all_touched=True, indexes=1)
        
        return clip, transform
        
    except Exception as e:
        logger.debug(f"Extraction failed: {e}")
        raise

# ---------------- Multi-temporal acquisition ----------------
def get_recent_tile_dates(tile_id: str, lookback_days: int = 30, limit: int = 3) -> List[str]:
    """Get multiple recent acquisition dates for temporal compositing."""
    try:
        cutoff_date = (datetime.datetime.utcnow() - datetime.timedelta(days=lookback_days)).isoformat()
        
        resp = supabase.table("satellite_tiles").select("acquisition_date").eq(
            "tile_id", tile_id
        ).eq("status", "ready").gte(
            "acquisition_date", cutoff_date
        ).order("acquisition_date", desc=True).limit(limit).execute()
        
        data = getattr(resp, "data", None) or (resp.get("data") if isinstance(resp, dict) else None)
        if data:
            return [rec.get("acquisition_date") for rec in data]
        return []
    except Exception as e:
        logger.debug(f"get_recent_tile_dates failed for {tile_id}: {e}")
        return []

# ---------------- Enhanced NDVI extraction ----------------
def extract_ndvi_from_tile(
    tile_id: str,
    acq_date: str,
    land_geom_4326: Dict,
    debug_land_id: str,
    enable_buffer: bool = True
) -> Optional[Tuple[np.ndarray, Dict[str, Any], str, Dict[str, Any]]]:
    """
    Enhanced NDVI extraction with buffering and interpolation for small parcels.
    Returns tuple (ndvi_array, metadata, tile_id, quality_info) or None on failure.
    """
    logger.info(f"ðŸ” Extracting NDVI: {tile_id}/{acq_date} for land {debug_land_id}")

    # Apply buffer for small parcels to capture edge pixels
    if enable_buffer:
        land_geom_buffered = buffer_geometry_meters(land_geom_4326, GEOMETRY_BUFFER_METERS)
        logger.debug(f"Applied {GEOMETRY_BUFFER_METERS}m buffer to geometry")
    else:
        land_geom_buffered = land_geom_4326

    ndvi_rel = f"tiles/ndvi/{tile_id}/{acq_date}/ndvi.tif"
    b04_rel = f"tiles/raw/{tile_id}/{acq_date}/B04.tif"
    b08_rel = f"tiles/raw/{tile_id}/{acq_date}/B08.tif"

    ndvi_url = get_signed_b2_url(ndvi_rel)
    b04_url = get_signed_b2_url(b04_rel)
    b08_url = get_signed_b2_url(b08_rel)

    quality_info = {
        "source": "unknown",
        "method": "unknown",
        "cloud_detected": False,
        "edge_pixels_used": False
    }

    # 1) Try precomputed NDVI COG
    if ndvi_url:
        try:
            src, env = _rasterio_open_with_retries(ndvi_url, {}, retries=2)
            try:
                logger.debug(f"NDVI tile opened: CRS={src.crs}, bounds={src.bounds}, nodata={src.nodata}")
                land_proj = reproject_to_raster_crs(land_geom_buffered, src.crs)
                
                try:
                    clip, transform = extract_with_interpolation(src, land_proj, use_bilinear=True)
                except ValueError as ve:
                    logger.debug(f"No overlap: {ve}")
                    src.close()
                    env.__exit__(None, None, None)
                    return None
                
                if clip.size == 0:
                    logger.debug("NDVI clip is empty")
                    src.close()
                    env.__exit__(None, None, None)
                    return None
                
                arr = clip[0] if clip.ndim > 2 else clip
                nod = src.nodata if src.nodata is not None else -9999
                arr = np.where(arr == nod, -1.0, arr).astype(np.float32)
                
                quality_info["source"] = "precomputed_cog"
                quality_info["method"] = "bilinear_interpolation"
                quality_info["edge_pixels_used"] = True
                
                src.close()
                env.__exit__(None, None, None)
                return arr, {"transform": transform, "crs": src.crs, "nodata": -1.0}, tile_id, quality_info
                
            except Exception:
                try:
                    src.close()
                except Exception:
                    pass
                env.__exit__(None, None, None)
                raise
        except RasterioIOError as e:
            logger.debug(f"Rasterio cannot open precomputed NDVI {ndvi_rel}: {e}")
        except Exception as e:
            logger.warning(f"Failed extraction from precomputed NDVI {ndvi_rel}: {e}")

    # 2) Fallback compute from B04/B08 with enhanced extraction
    if b04_url and b08_url:
        try:
            red_src, red_env = _rasterio_open_with_retries(b04_url, {}, retries=2)
            nir_src, nir_env = _rasterio_open_with_retries(b08_url, {}, retries=2)
            try:
                if str(red_src.crs) != str(nir_src.crs):
                    logger.debug("Band CRSs differ; continuing with red CRS")
                
                land_proj = reproject_to_raster_crs(land_geom_buffered, red_src.crs)
                
                red_clip, transform = extract_with_interpolation(red_src, land_proj, use_bilinear=True)
                nir_clip, _ = extract_with_interpolation(nir_src, land_proj, use_bilinear=True)
                
                if red_clip.size == 0 or nir_clip.size == 0:
                    logger.debug("Band clips empty")
                    red_src.close(); nir_src.close()
                    red_env.__exit__(None, None, None); nir_env.__exit__(None, None, None)
                    return None
                
                red = red_clip[0].astype(np.float32) if red_clip.ndim > 2 else red_clip.astype(np.float32)
                nir = nir_clip[0].astype(np.float32) if nir_clip.ndim > 2 else nir_clip.astype(np.float32)
                
                # Handle nodata values
                red_nodata = red_src.nodata if red_src.nodata is not None else 0
                nir_nodata = nir_src.nodata if nir_src.nodata is not None else 0
                
                red = np.where(red == red_nodata, np.nan, red)
                nir = np.where(nir == nir_nodata, np.nan, nir)
                
                # Calculate NDVI
                np.seterr(divide="ignore", invalid="ignore")
                denom = nir + red
                ndvi = np.where(denom != 0, (nir - red) / denom, np.nan)
                ndvi = np.clip(ndvi, -1.0, 1.0)
                ndvi = np.where(np.isnan(ndvi), -1.0, ndvi).astype(np.float32)
                
                quality_info["source"] = "on_the_fly_computation"
                quality_info["method"] = "bilinear_interpolation_bands"
                quality_info["edge_pixels_used"] = True
                
                red_src.close(); nir_src.close()
                red_env.__exit__(None, None, None); nir_env.__exit__(None, None, None)
                
                return ndvi, {"transform": transform, "crs": red_src.crs, "nodata": -1.0}, tile_id, quality_info
                
            except Exception:
                try:
                    red_src.close(); nir_src.close()
                except Exception:
                    pass
                red_env.__exit__(None, None, None); nir_env.__exit__(None, None, None)
                raise
        except Exception as e:
            logger.error(f"Failed to compute NDVI from bands for {tile_id}/{acq_date}: {e}")

    logger.debug(f"No NDVI available for {tile_id}/{acq_date}")
    return None

# ---------------- Multi-temporal merging ----------------
def merge_temporal_ndvi(
    ndvi_results: List[Tuple[np.ndarray, Dict, str, Dict]]
) -> Tuple[np.ndarray, List[str], Dict]:
    """
    Merge multiple temporal NDVI results using median compositing.
    This reduces noise and cloud contamination for small parcels.
    """
    if not ndvi_results:
        raise ValueError("No ndvi_results to merge")
    
    arrays = [r[0] for r in ndvi_results]
    dates = [r[2] for r in ndvi_results]  # tile_id actually, but represents temporal info
    quality_infos = [r[3] for r in ndvi_results]
    
    if len(arrays) == 1:
        return arrays[0].astype(np.float32), dates, quality_infos[0]
    
    logger.info(f"Merging {len(arrays)} temporal NDVI images using median composite")
    
    # Align shapes
    max_rows = max(a.shape[0] for a in arrays)
    max_cols = max(a.shape[1] for a in arrays)
    padded = []
    for a in arrays:
        rows, cols = a.shape
        pad_rows = max_rows - rows
        pad_cols = max_cols - cols
        pad = ((0, pad_rows), (0, pad_cols))
        padded.append(np.pad(a, pad, mode="constant", constant_values=-1.0))
    
    stack = np.stack(padded, axis=0).astype(np.float32)  # shape (n, r, c)
    valid_mask = stack != -1.0
    
    # Use median instead of mean for better outlier rejection
    with np.errstate(invalid="ignore", divide="ignore"):
        # Create masked array
        masked_stack = np.ma.masked_where(~valid_mask, stack)
        median = np.ma.median(masked_stack, axis=0)
        result = np.where(median.mask, -1.0, median.data).astype(np.float32)
    
    # Merge quality info
    merged_quality = {
        "source": "multi_temporal_composite",
        "method": "median_compositing",
        "temporal_images_used": len(arrays),
        "edge_pixels_used": any(qi.get("edge_pixels_used", False) for qi in quality_infos)
    }
    
    return result, dates, merged_quality

def merge_multi_tile_ndvi(ndvi_results: List[Tuple[np.ndarray, Dict, str, Dict]]) -> Tuple[np.ndarray, Dict]:
    """
    Merge multiple spatial tile NDVI results into a single array.
    Uses weighted average based on valid pixel coverage.
    """
    if not ndvi_results:
        raise ValueError("No ndvi_results to merge")
    
    arrays = [r[0] for r in ndvi_results]
    quality_infos = [r[3] for r in ndvi_results]
    
    if len(arrays) == 1:
        return arrays[0].astype(np.float32), quality_infos[0]
    
    # Determine max shape
    max_rows = max(a.shape[0] for a in arrays)
    max_cols = max(a.shape[1] for a in arrays)
    padded = []
    for a in arrays:
        rows, cols = a.shape
        pad_rows = max_rows - rows
        pad_cols = max_cols - cols
        pad = ((0, pad_rows), (0, pad_cols))
        padded.append(np.pad(a, pad, mode="constant", constant_values=-1.0))
    
    stack = np.stack(padded, axis=0).astype(np.float32)
    valid_mask = stack != -1.0
    
    with np.errstate(invalid="ignore", divide="ignore"):
        numerator = np.where(valid_mask, stack, np.nan).sum(axis=0)
        counts = valid_mask.sum(axis=0)
        mean = np.where(counts > 0, numerator / counts, -1.0)
    
    merged_quality = {
        "source": "multi_tile_merge",
        "method": "weighted_average",
        "tiles_merged": len(arrays),
        "edge_pixels_used": any(qi.get("edge_pixels_used", False) for qi in quality_infos)
    }
    
    return mean.astype(np.float32), merged_quality

# ---------------- Enhanced statistics with quality metrics ----------------
def detect_edge_effects(ndvi: np.ndarray, valid_mask: np.ndarray) -> float:
    """
    Detect edge pixel ratio - higher ratio indicates more boundary effects.
    Edge pixels are those on the perimeter of valid data.
    """
    if valid_mask.sum() == 0:
        return 0.0
    
    try:
        from scipy import ndimage
        # Erode valid mask by 1 pixel
        eroded = ndimage.binary_erosion(valid_mask)
        edge_pixels = valid_mask & ~eroded
        edge_ratio = edge_pixels.sum() / valid_mask.sum()
        return float(edge_ratio)
    except ImportError:
        # Fallback without scipy
        return 0.0

def calculate_enhanced_statistics(ndvi: np.ndarray, quality_info: Dict) -> Dict[str, Any]:
    """
    Calculate comprehensive statistics with quality metrics and uncertainty estimation.
    """
    valid_mask = (ndvi != -1.0) & (ndvi >= -1.0) & (ndvi <= 1.0) & ~np.isnan(ndvi)
    valid_pixels = ndvi[valid_mask]
    total_pixels = int(ndvi.size)
    
    if valid_pixels.size == 0:
        return {
            "mean": None,
            "median": None,
            "min": None,
            "max": None,
            "std": None,
            "percentile_10": None,
            "percentile_90": None,
            "valid_pixels": 0,
            "total_pixels": total_pixels,
            "coverage": 0.0,
            "confidence_level": "insufficient_data",
            "quality_score": 0.0,
            "uncertainty_estimate": 1.0
        }
    
    # Basic statistics
    mean_val = float(np.mean(valid_pixels))
    median_val = float(np.median(valid_pixels))
    std_val = float(np.std(valid_pixels))
    
    # Percentiles for robust statistics
    p10 = float(np.percentile(valid_pixels, 10))
    p90 = float(np.percentile(valid_pixels, 90))
    
    # Coverage
    coverage = float(valid_pixels.size / total_pixels * 100.0)
    
    # Confidence level based on pixel count
    if valid_pixels.size >= CONFIDENCE_HIGH_PIXELS:
        confidence = "high"
        confidence_score = 1.0
    elif valid_pixels.size >= CONFIDENCE_MEDIUM_PIXELS:
        confidence = "medium"
        confidence_score = 0.7
    else:
        confidence = "low"
        confidence_score = 0.4
    
    # Edge effect detection
    edge_ratio = detect_edge_effects(ndvi, valid_mask)
    
    # Quality score (0-100)
    # Based on: pixel count, coverage, std deviation, edge effects
    pixel_score = min(valid_pixels.size / CONFIDENCE_HIGH_PIXELS, 1.0) * 40
    coverage_score = min(coverage / 50.0, 1.0) * 30
    stability_score = max(0, 1.0 - std_val) * 20  # Lower std = more stable
    edge_score = max(0, 1.0 - edge_ratio) * 10  # Lower edge ratio = better
    
    quality_score = pixel_score + coverage_score + stability_score + edge_score
    
    # Uncertainty estimate (higher = less certain)
    # Based on Sentinel-2 documented uncertainty + small parcel effects
    base_uncertainty = 0.05  # Â±0.05 NDVI units (Sentinel-2 spec)
    pixel_uncertainty = 0.10 / max(1, np.sqrt(valid_pixels.size / 25))  # Decreases with more pixels
    edge_uncertainty = edge_ratio * 0.05  # Edge effects add uncertainty
    total_uncertainty = base_uncertainty + pixel_uncertainty + edge_uncertainty
    
    # Quality flags
    quality_flags = []
    if valid_pixels.size < CONFIDENCE_MEDIUM_PIXELS:
        quality_flags.append("small_sample_size")
    if edge_ratio > 0.5:
        quality_flags.append("high_edge_effects")
    if std_val > 0.3:
        quality_flags.append("high_variability")
    if coverage < 30:
        quality_flags.append("low_spatial_coverage")
    if quality_info.get("source") == "on_the_fly_computation":
        quality_flags.append("computed_from_bands")
    
    return {
        "mean": mean_val,
        "median": median_val,
        "min": float(np.min(valid_pixels)),
        "max": float(np.max(valid_pixels)),
        "std": std_val,
        "percentile_10": p10,
        "percentile_90": p90,
        "valid_pixels": int(valid_pixels.size),
        "total_pixels": total_pixels,
        "coverage": coverage,
        "confidence_level": confidence,
        "confidence_score": confidence_score,
        "quality_score": quality_score,
        "edge_pixel_ratio": edge_ratio,
        "uncertainty_estimate": total_uncertainty,
        "quality_flags": quality_flags,
        "processing_metadata": quality_info
    }

def create_quality_metrics(stats: Dict, quality_info: Dict, dates: List[str]) -> QualityMetrics:
    """Create comprehensive quality metrics object for audit compliance."""
    
    # Determine temporal consistency if multiple dates
    temporal_consistency = None
    if len(dates) > 1:
        temporal_consistency = 1.0 - min(stats.get("std", 0.3) / 0.3, 1.0)  # Normalized
    
    return QualityMetrics(
        confidence_level=stats["confidence_level"],
        pixel_count=stats["valid_pixels"],
        edge_pixel_ratio=stats["edge_pixel_ratio"],
        spatial_coverage=stats["coverage"],
        temporal_consistency=temporal_consistency,
        cloud_contamination=0.0,  # Would be calculated from cloud mask if available
        quality_score=stats["quality_score"],
        quality_flags=stats["quality_flags"],
        data_source="Sentinel-2 L2A",
        processing_method=quality_info.get("method", "standard_extraction"),
        uncertainty_estimate=stats["uncertainty_estimate"]
    )

# ---------------- Enhanced visualization ----------------
def create_enhanced_thumbnail(ndvi_array: np.ndarray, stats: Dict, max_size: int = 512) -> bytes:
    """
    Create enhanced colorized thumbnail with quality indicators.
    Uses adaptive colormap based on data distribution.
    """
    # Normalize NDVI to 0-1 range for colormap
    norm = np.clip((ndvi_array + 1.0) / 2.0, 0.0, 1.0)
    
    # Use RdYlGn colormap (Red-Yellow-Green) standard for vegetation
    cmap = cm.get_cmap("RdYlGn")
    rgba = (cmap(norm) * 255).astype(np.uint8)
    
    # Set alpha channel: transparent for nodata, opaque for valid
    alpha = np.where(ndvi_array == -1.0, 0, 255).astype(np.uint8)
    rgba[..., 3] = alpha
    
    # Create image
    img = Image.fromarray(rgba, mode="RGBA")
    
    # Add quality indicator border if needed
    if stats.get("confidence_level") == "low":
        # Add visual indicator for low confidence
        from PIL import ImageDraw
        draw = ImageDraw.Draw(img)
        width, height = img.size
        border_color = (255, 165, 0, 255)  # Orange for low confidence
        border_width = max(2, min(width, height) // 100)
        for i in range(border_width):
            draw.rectangle([i, i, width-1-i, height-1-i], outline=border_color)
    
    # Resize if needed
    if max(img.size) > max_size:
        img.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)
    
    # Save to bytes
    buf = io.BytesIO()
    img.save(buf, format="PNG", optimize=True)
    buf.seek(0)
    return buf.getvalue()

def upload_thumbnail_to_supabase_sync(land_id: str, date: str, png_bytes: bytes) -> Optional[str]:
    """Upload thumbnail to Supabase storage. Returns public URL or None."""
    try:
        path = f"{land_id}/{date}/ndvi_colorized.png"
        file_obj = io.BytesIO(png_bytes)
        file_obj.seek(0)
        
        res = supabase.storage.from_(SUPABASE_NDVI_BUCKET).upload(
            path=path,
            file=file_obj,
            file_options={"content_type": "image/png", "upsert": True}
        )
        
        if isinstance(res, dict) and res.get("error"):
            logger.error(f"Supabase upload error: {res.get('error')}")
            return None
        
        public_url = f"{SUPABASE_URL}/storage/v1/object/public/{SUPABASE_NDVI_BUCKET}/{path}"
        logger.info(f"Uploaded thumbnail for {land_id} {date} -> {public_url}")
        return public_url
    except Exception as e:
        logger.error(f"Failed to upload thumbnail: {e}")
        return None

# ---------------- DB helpers ----------------
def get_latest_tile_date_sync(tile_id: str) -> Optional[str]:
    try:
        resp = supabase.table("satellite_tiles").select("acquisition_date").eq(
            "tile_id", tile_id
        ).eq("status", "ready").order("acquisition_date", desc=True).limit(1).execute()
        
        data = getattr(resp, "data", None) or (resp.get("data") if isinstance(resp, dict) else None)
        if data:
            rec = data[0]
            return rec.get("acquisition_date")
        return None
    except Exception as e:
        logger.debug(f"get_latest_tile_date_sync failed for {tile_id}: {e}")
        return None

def upsert_ndvi_data_sync(record: Dict) -> None:
    try:
        supabase.table("ndvi_data").upsert(record).execute()
    except Exception as e:
        logger.error(f"ndvi_data upsert failed: {e}")

def upsert_micro_tile_sync(record: Dict) -> None:
    try:
        supabase.table("ndvi_micro_tiles").upsert(record).execute()
    except Exception as e:
        logger.error(f"ndvi_micro_tiles upsert failed: {e}")

def update_land_sync(land_id: str, payload: Dict) -> None:
    try:
        supabase.table("lands").update(payload).eq("id", land_id).execute()
    except Exception as e:
        logger.error(f"lands update failed: {land_id} - {e}")

def insert_processing_log_sync(record: Dict) -> None:
    try:
        supabase.table("ndvi_processing_logs").insert(record).execute()
    except Exception as e:
        logger.error(f"processing_log insert failed: {e}")

# ---------------- Main single land processing (async-friendly) ----------------
async def process_single_land_async(
    land: Dict,
    tile_ids: Optional[List[str]],
    acquisition_date_override: Optional[str],
    executor: ThreadPoolExecutor
) -> Dict[str, Any]:
    """
    Enhanced land processing with multi-temporal compositing and comprehensive quality metrics.
    Designed for small landholdings with audit-ready documentation.
    """
    loop = asyncio.get_running_loop()
    land_id = land.get("id")
    tenant_id = land.get("tenant_id")
    
    out = {
        "land_id": land_id,
        "success": False,
        "error": None,
        "stats": None,
        "thumbnail_url": None,
        "quality_metrics": None,
        "processing_notes": []
    }
    
    try:
        geometry = extract_geometry_from_land(land)
        if not geometry:
            raise ValueError("No valid geometry for land")
        
        # Determine tiles
        if tile_ids:
            tiles_to_process = tile_ids
        elif land.get("tile_ids"):
            tiles_to_process = land["tile_ids"]
        else:
            try:
                resp = supabase.rpc("get_intersecting_tiles", {"land_geom": json.dumps(geometry)}).execute()
                data = getattr(resp, "data", None) or (resp.get("data") if isinstance(resp, dict) else None)
                tiles_to_process = [t["tile_id"] for t in (data or [])]
            except Exception as e:
                logger.debug(f"RPC get_intersecting_tiles failed: {e}")
                tiles_to_process = []
        
        if not tiles_to_process:
            raise ValueError("No intersecting tiles found")
        
        out["processing_notes"].append(f"Processing {len(tiles_to_process)} intersecting tiles")
        
        # Multi-temporal extraction if enabled
        all_extractions = []
        acquisition_dates_used = []
        
        for tile_id in tiles_to_process:
            if ENABLE_MULTI_TEMPORAL and not acquisition_date_override:
                # Get multiple recent dates for this tile
                recent_dates = await loop.run_in_executor(
                    executor, get_recent_tile_dates, tile_id, TEMPORAL_LOOKBACK_DAYS, MAX_TEMPORAL_IMAGES
                )
                
                if recent_dates:
                    out["processing_notes"].append(f"Tile {tile_id}: using {len(recent_dates)} temporal images")
                    for acq_date in recent_dates:
                        extraction = await loop.run_in_executor(
                            executor, extract_ndvi_from_tile, tile_id, acq_date, geometry, land_id, True
                        )
                        if extraction:
                            all_extractions.append(extraction)
                            acquisition_dates_used.append(acq_date)
                else:
                    # Fall back to single latest
                    acq_date = await loop.run_in_executor(executor, get_latest_tile_date_sync, tile_id)
                    if acq_date:
                        extraction = await loop.run_in_executor(
                            executor, extract_ndvi_from_tile, tile_id, acq_date, geometry, land_id, True
                        )
                        if extraction:
                            all_extractions.append(extraction)
                            acquisition_dates_used.append(acq_date)
            else:
                # Single date extraction
                acq_date = acquisition_date_override or await loop.run_in_executor(
                    executor, get_latest_tile_date_sync, tile_id
                )
                if not acq_date:
                    logger.debug(f"No acquisition date for {tile_id}")
                    continue
                
                extraction = await loop.run_in_executor(
                    executor, extract_ndvi_from_tile, tile_id, acq_date, geometry, land_id, True
                )
                if extraction:
                    all_extractions.append(extraction)
                    acquisition_dates_used.append(acq_date)
        
        if not all_extractions:
            # Try without buffer as last resort
            out["processing_notes"].append("Retrying without geometry buffer")
            for tile_id in tiles_to_process:
                acq_date = acquisition_date_override or await loop.run_in_executor(
                    executor, get_latest_tile_date_sync, tile_id
                )
                if acq_date:
                    extraction = await loop.run_in_executor(
                        executor, extract_ndvi_from_tile, tile_id, acq_date, geometry, land_id, False
                    )
                    if extraction:
                        all_extractions.append(extraction)
                        acquisition_dates_used.append(acq_date)
        
        if not all_extractions:
            raise ValueError("No NDVI data extracted from any intersecting tiles")
        
        out["processing_notes"].append(f"Successfully extracted {len(all_extractions)} NDVI datasets")
        
        # Merge temporal and spatial data
        if len(all_extractions) > 1 and ENABLE_MULTI_TEMPORAL:
            # First merge temporal, then spatial
            merged_ndvi, dates_used, quality_info = await loop.run_in_executor(
                executor, merge_temporal_ndvi, all_extractions
            )
        else:
            # Just merge spatial tiles
            if len(all_extractions) > 1:
                merged_ndvi, quality_info = await loop.run_in_executor(
                    executor, merge_multi_tile_ndvi, all_extractions
                )
            else:
                merged_ndvi = all_extractions[0][0]
                quality_info = all_extractions[0][3]
            dates_used = acquisition_dates_used
        
        # Calculate enhanced statistics
        stats = await loop.run_in_executor(
            executor, calculate_enhanced_statistics, merged_ndvi, quality_info
        )
        
        # Check minimum pixel threshold
        if stats["valid_pixels"] < MIN_VALID_PIXELS:
            out["processing_notes"].append(
                f"âš ï¸ Only {stats['valid_pixels']} valid pixels (minimum {MIN_VALID_PIXELS}). "
                "Results should be treated as indicative only."
            )
            # Don't fail, but flag as low confidence
            stats["quality_flags"].append("below_minimum_threshold")
            stats["confidence_level"] = "insufficient_data"
        
        # Create quality metrics
        quality_metrics = create_quality_metrics(stats, quality_info, dates_used)
        
        # Generate thumbnail with quality indicators
        date_for_record = acquisition_date_override or datetime.date.today().isoformat()
        thumbnail_bytes = await loop.run_in_executor(
            executor, create_enhanced_thumbnail, merged_ndvi, stats
        )
        thumbnail_url = await loop.run_in_executor(
            executor, upload_thumbnail_to_supabase_sync, land_id, date_for_record, thumbnail_bytes
        )
        
        # Prepare audit-compliant documentation
        processing_metadata = {
            "processing_version": "10.0",
            "satellite_source": "Sentinel-2 L2A (ESA)",
            "spatial_resolution": "10m",
            "temporal_coverage": dates_used,
            "atmospheric_correction": "Sen2Cor",
            "geometry_buffer_applied": f"{GEOMETRY_BUFFER_METERS}m",
            "interpolation_method": quality_info.get("method", "bilinear"),
            "statistical_method": "median_composite" if len(all_extractions) > 1 else "single_acquisition",
            "quality_assurance": {
                "confidence_level": quality_metrics.confidence_level,
                "quality_score": quality_metrics.quality_score,
                "uncertainty_estimate": f"Â±{quality_metrics.uncertainty_estimate:.3f} NDVI units",
                "quality_flags": quality_metrics.quality_flags
            },
            "regulatory_notes": {
                "suitable_for": "Agricultural monitoring, vegetation trend analysis",
                "limitations": "Results for parcels <1 hectare should be considered indicative",
                "validation_recommendation": "Field verification recommended for critical decisions",
                "accuracy_statement": f"NDVI accuracy: Â±{quality_metrics.uncertainty_estimate:.3f} units (95% confidence)"
            }
        }
        
        # Database records with comprehensive metadata
        ndvi_record = {
            "tenant_id": tenant_id,
            "land_id": land_id,
            "date": date_for_record,
            "mean_ndvi": stats["mean"],
            "median_ndvi": stats["median"],
            "min_ndvi": stats["min"],
            "max_ndvi": stats["max"],
            "ndvi_std": stats["std"],
            "percentile_10": stats["percentile_10"],
            "percentile_90": stats["percentile_90"],
            "valid_pixels": stats["valid_pixels"],
            "coverage_percentage": stats["coverage"],
            "confidence_level": stats["confidence_level"],
            "quality_score": stats["quality_score"],
            "edge_pixel_ratio": stats["edge_pixel_ratio"],
            "uncertainty_estimate": stats["uncertainty_estimate"],
            "quality_flags": json.dumps(stats["quality_flags"]),
            "processing_metadata": json.dumps(processing_metadata),
            "image_url": thumbnail_url,
            "created_at": now_iso(),
            "computed_at": now_iso()
        }
        
        micro_tile_record = {
            "tenant_id": tenant_id,
            "land_id": land_id,
            "acquisition_date": date_for_record,
            "ndvi_mean": stats["mean"],
            "ndvi_median": stats["median"],
            "ndvi_min": stats["min"],
            "ndvi_max": stats["max"],
            "ndvi_std_dev": stats["std"],
            "confidence_level": stats["confidence_level"],
            "quality_score": stats["quality_score"],
            "valid_pixel_count": stats["valid_pixels"],
            "ndvi_thumbnail_url": thumbnail_url,
            "bbox": geometry,
            "cloud_cover": 0,
            "processing_metadata": json.dumps(processing_metadata),
            "created_at": now_iso()
        }
        
        # Upsert DB rows in executor
        await loop.run_in_executor(executor, upsert_ndvi_data_sync, ndvi_record)
        await loop.run_in_executor(executor, upsert_micro_tile_sync, micro_tile_record)
        await loop.run_in_executor(executor, update_land_sync, land_id, {
            "last_ndvi_value": stats["mean"],
            "last_ndvi_median": stats["median"],
            "last_ndvi_calculation": date_for_record,
            "ndvi_confidence_level": stats["confidence_level"],
            "ndvi_quality_score": stats["quality_score"],
            "ndvi_thumbnail_url": thumbnail_url,
            "updated_at": now_iso()
        })
        
        # Log successful processing with quality info
        log_message = (
            f"âœ… Land {land_id} processed successfully\n"
            f"   Mean NDVI: {stats['mean']:.3f} (Â±{stats['uncertainty_estimate']:.3f})\n"
            f"   Median: {stats['median']:.3f}, Range: [{stats['min']:.3f}, {stats['max']:.3f}]\n"
            f"   Pixels: {stats['valid_pixels']}, Coverage: {stats['coverage']:.1f}%\n"
            f"   Confidence: {stats['confidence_level'].upper()}, Quality Score: {stats['quality_score']:.1f}/100\n"
            f"   Temporal images: {len(dates_used)}, Dates: {', '.join(dates_used[:3])}"
        )
        logger.info(log_message)
        
        if stats["quality_flags"]:
            logger.warning(f"   Quality flags: {', '.join(stats['quality_flags'])}")
        
        out.update({
            "success": True,
            "stats": stats,
            "thumbnail_url": thumbnail_url,
            "quality_metrics": quality_metrics.__dict__,
            "processing_metadata": processing_metadata
        })
        
    except Exception as e:
        tb = traceback.format_exc()
        logger.error(f"âŒ Land {land_id} failed: {e}\n{tb}")
        out["error"] = str(e)
        out["processing_notes"].append(f"Error: {str(e)}")
        
        # Insert processing log
        try:
            await loop.run_in_executor(executor, insert_processing_log_sync, {
                "tenant_id": land.get("tenant_id"),
                "land_id": land_id,
                "processing_step": "ndvi_extraction",
                "step_status": "failed",
                "error_message": str(e)[:500],
                "error_details": {
                    "traceback": tb[:1000],
                    "processing_notes": out["processing_notes"]
                },
                "created_at": now_iso()
            })
        except Exception:
            pass
    
    return out

# ---------------- Orchestrator (async) ----------------
async def process_request_async(
    queue_id: str,
    tenant_id: str,
    land_ids: List[str],
    tile_ids: Optional[List[str]] = None
) -> Dict[str, Any]:
    """Process NDVI request with enhanced quality control for small landholdings."""
    logger.info(f"ðŸš€ Queue {queue_id}: processing {len(land_ids)} lands for tenant {tenant_id}")
    logger.info(f"   Multi-temporal: {ENABLE_MULTI_TEMPORAL}, Buffer: {GEOMETRY_BUFFER_METERS}m")
    
    start_ts = time.time()
    
    try:
        resp = supabase.table("lands").select("*").eq("tenant_id", tenant_id).in_("id", land_ids).execute()
        lands = getattr(resp, "data", None) or (resp.get("data") if isinstance(resp, dict) else None) or []
    except Exception as e:
        logger.error(f"Failed to fetch lands: {e}")
        lands = []
    
    if not lands:
        logger.warning("No lands found for processing")
        return {
            "queue_id": queue_id,
            "processed_count": 0,
            "failed_count": len(land_ids),
            "results": [],
            "duration_ms": 0
        }
    
    executor = ThreadPoolExecutor(max_workers=THREAD_POOL_WORKERS)
    sem = asyncio.Semaphore(MAX_CONCURRENT_LANDS)
    
    async def _proc(land):
        async with sem:
            return await process_single_land_async(land, tile_ids, None, executor)
    
    tasks = [asyncio.create_task(_proc(land)) for land in lands]
    results = await asyncio.gather(*tasks, return_exceptions=False)
    
    processed = sum(1 for r in results if r.get("success"))
    failed = [r for r in results if not r.get("success")]
    duration_ms = int((time.time() - start_ts) * 1000)
    
    # Aggregate quality metrics
    quality_summary = {
        "high_confidence": sum(1 for r in results if r.get("quality_metrics", {}).get("confidence_level") == "high"),
        "medium_confidence": sum(1 for r in results if r.get("quality_metrics", {}).get("confidence_level") == "medium"),
        "low_confidence": sum(1 for r in results if r.get("quality_metrics", {}).get("confidence_level") == "low"),
        "avg_quality_score": np.mean([r.get("stats", {}).get("quality_score", 0) for r in results if r.get("success")]) if processed > 0 else 0
    }
    
    final_status = "completed" if processed > 0 else "failed"
    
    try:
        supabase.table("ndvi_request_queue").update({
            "status": final_status,
            "processed_count": processed,
            "failed_count": len(failed),
            "processing_duration_ms": duration_ms,
            "quality_summary": json.dumps(quality_summary),
            "completed_at": now_iso()
        }).eq("id", queue_id).execute()
    except Exception as e:
        logger.error(f"Failed to update ndvi_request_queue: {e}")
    
    logger.info(f"âœ… Queue {queue_id} completed: {processed} succeeded, {len(failed)} failed")
    logger.info(f"   Quality: {quality_summary['high_confidence']} high, {quality_summary['medium_confidence']} medium, {quality_summary['low_confidence']} low confidence")
    logger.info(f"   Average quality score: {quality_summary['avg_quality_score']:.1f}/100")
    
    return {
        "queue_id": queue_id,
        "processed_count": processed,
        "failed_count": len(failed),
        "quality_summary": quality_summary,
        "results": results,
        "duration_ms": duration_ms
    }

# ---------------- Cron runner ----------------
def run_cron(limit: int = 10, max_retries: int = 3):
    """Run cron job to process queued NDVI requests."""
    logger.info("ðŸ”„ NDVI Worker Cron Start (v10.0 - Enhanced for Small Landholdings)")
    
    try:
        queue_resp = supabase.table("ndvi_request_queue").select("*").eq(
            "status", "queued"
        ).order("created_at", desc=False).limit(limit).execute()
        items = getattr(queue_resp, "data", None) or (queue_resp.get("data") if isinstance(queue_resp, dict) else None) or []
    except Exception as e:
        logger.error(f"Failed to fetch queue items: {e}")
        items = []
    
    if not items:
        logger.info("No queued items")
        return
    
    logger.info(f"Processing {len(items)} queued requests")
    
    async def _handle_item(item):
        queue_id = item["id"]
        tenant_id = item["tenant_id"]
        land_ids = item.get("land_ids", [])
        tile_id_single = item.get("tile_id")
        retry_count = item.get("retry_count", 0)
        
        if retry_count >= max_retries:
            logger.warning(f"Max retries for queue {queue_id}")
            supabase.table("ndvi_request_queue").update({
                "status": "failed",
                "last_error": f"Max retries ({max_retries}) exceeded",
                "completed_at": now_iso()
            }).eq("id", queue_id).execute()
            return
        
        supabase.table("ndvi_request_queue").update({
            "status": "processing",
            "started_at": now_iso(),
            "retry_count": retry_count + 1
        }).eq("id", queue_id).execute()
        
        try:
            await process_request_async(
                queue_id, tenant_id, land_ids,
                [tile_id_single] if tile_id_single else None
            )
        except Exception as e:
            logger.exception(f"Queue {queue_id} failed: {e}")
            supabase.table("ndvi_request_queue").update({
                "status": "queued",
                "last_error": str(e)[:500]
            }).eq("id", queue_id).execute()
    
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    tasks = [_handle_item(item) for item in items]
    loop.run_until_complete(asyncio.gather(*tasks))
    loop.close()
    
    logger.info("âœ… Cron run completed")

# ---------------- CLI / Entrypoint ----------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="NDVI Land Worker v10.0 - Enhanced for Small Landholdings"
    )
    parser.add_argument("--mode", choices=["cron", "single"], default="cron")
    parser.add_argument("--limit", type=int, default=5)
    parser.add_argument("--queue-id", type=str, help="process specific queue id (single mode)")
    args = parser.parse_args()
    
    logger.info("=" * 80)
    logger.info(f"NDVI Worker v10.0 Starting (mode={args.mode})")
    logger.info("Enhanced Features:")
    logger.info(f"  â€¢ Geometry buffering: {GEOMETRY_BUFFER_METERS}m for edge pixel capture")
    logger.info(f"  â€¢ Bilinear interpolation: Enabled for small parcels")
    logger.info(f"  â€¢ Multi-temporal compositing: {ENABLE_MULTI_TEMPORAL}")
    logger.info(f"  â€¢ Quality scoring: Comprehensive audit-ready metrics")
    logger.info(f"  â€¢ Minimum pixel threshold: {MIN_VALID_PIXELS} (with warnings)")
    logger.info("=" * 80)
    
    if args.mode == "single" and args.queue_id:
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

            try:
                queue_item = supabase.table("ndvi_request_queue").select("*").eq(
                    "id", args.queue_id
                ).single().execute()

                item = getattr(queue_item, "data", None) or (
                    queue_item.get("data") if isinstance(queue_item, dict) else None
                )

                if not item:
                    logger.error(f"âŒ Queue item {args.queue_id} not found in database.")
                else:
                    tenant_id = item.get("tenant_id")
                    land_ids = item.get("land_ids", [])
                    tile_id_single = item.get("tile_id")
                    logger.info(
                        f"ðŸ” Running single queue: {args.queue_id} | "
                        f"Tenant={tenant_id} | Lands={len(land_ids)}"
                    )

                    # Mark as processing
                    try:
                        supabase.table("ndvi_request_queue").update({
                            "status": "processing",
                            "started_at": now_iso(),
                            "retry_count": (item.get("retry_count") or 0) + 1
                        }).eq("id", args.queue_id).execute()
                    except Exception as e:
                        logger.warning(f"Could not update queue status to processing: {e}")

                    # Main async execution
                    try:
                        result = loop.run_until_complete(
                            process_request_async(
                                args.queue_id,
                                tenant_id,
                                land_ids,
                                [tile_id_single] if tile_id_single else None
                            )
                        )
                        logger.info("âœ… Single queue processing completed")
                        logger.info(json.dumps(result.get("quality_summary", {}), indent=2))
                    except Exception as e:
                        logger.exception(f"Single queue processing failed: {e}")
                        try:
                            supabase.table("ndvi_request_queue").update({
                                "status": "failed",
                                "last_error": str(e)[:500],
                                "completed_at": now_iso()
                            }).eq("id", args.queue_id).execute()
                        except Exception:
                            pass
            except Exception as e:
                logger.exception(f"Single mode setup failed: {e}")

        except Exception as e:
            logger.critical(f"ðŸ’¥ Fatal error in single mode: {e}")
        finally:
            try:
                loop.close()
            except Exception:
                pass

    elif args.mode == "cron":
        try:
            run_cron(limit=args.limit)
        except Exception as e:
            logger.exception(f"Cron run failed: {e}")

    else:
        logger.error(f"Unknown mode: {args.mode}")
